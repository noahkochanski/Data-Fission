{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fission\n",
    "## Conducting Inference\n",
    "We conduct inference on some vector $Y$ given a set of covariates $X$ and a known covariance matrix  $\\Sigma = \\sigma^2I_n$ as follows:\n",
    "1. Decompose $y_i$ into $f(y_i) = y_i - Z_i$ and $g(y_i) = y_i + Z_i$ where $Z_i\\sim\\mathcal{N}(0,\\sigma^2)$\n",
    "2. Fit $f(y_i)$ using LASSO to select features, denoted as $M\\subseteq [p]$ (tuning parameter $\\lambda$ by 1 standard deviation rule)\n",
    "3. Fit $g(y_i)$ by linear regression without regularization using only the selected features.\n",
    "4. Construct CIs for the coeffecients trained in step 3. each at level $\\alpha$ using Theorem 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorem 2\n",
    "Let \n",
    "$$\\hat\\beta(M) = \\argmin_{\\tilde\\beta} = || g(Y) - X_M\\tilde\\beta||^2 = \\left( X_M^\\top X_M \\right)^{-1} X_M^\\top g(Y)$$\n",
    "\n",
    "and for $\\mu = \\mathbb{E}[Y\\mid X]\\in\\mathbb{R}^n$ (which is a fixed unknown quantity), \n",
    "\n",
    "$$\\beta^*(M) = \\argmin_{\\tilde\\beta} = \\mathbb{E}\\left[ || Y - X_M\\tilde\\beta ||^2 \\right] = \\left( X_M^\\top X_M \\right)^{-1} X_M^\\top \\mu.$$\n",
    "\n",
    "Then, \n",
    "\n",
    "$$\\hat\\beta(M) \\sim \\mathcal{N} \\left( \\beta^*(M), \\left( 1 +\\tau^{-2} \\right)\\left( X_M^\\top X_M\\right)^{-1} X_M^\\top \\Sigma X_M \\left( X_M^\\top X_M\\right)^{-1} \\right)$$\n",
    "\n",
    "Furthermore, we can form a $1-\\alpha$ CI for the $k$th element of $\\beta^*(M)$ as \n",
    "\n",
    "$$\\hat\\beta(M)\\pm z_{\\alpha/2}\\sqrt{\\left( 1 + \\tau^{-2} \\right) \\left[ \\left( X^\\top_M X_M\\right)^{-1} X_M^\\top\\Sigma X_M \\left( X_M^\\top X_M\\right)^{-1} \\right]_{kk} }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Setup: \n",
    "\n",
    "\"We choose $\\sigma^2 = 1$ and generate $n = 16$ data points with $p = 20$ covariates. For the first 15 data points, we have an associated vector of covariates  $x_i\\in\\mathbb{R}^p$ generated from independent Gaussians. The last data point, which we denote $x_\\text{lev}$, is generated in such a way as to ensure it is likely to bemore influential than the remaining observations due to having much larger leverage. We define \n",
    "\n",
    "$$x_\\text{lev} = \\gamma\\left( \\vert X_1 \\vert_\\infty, \\dots, \\vert X_p \\vert_\\infty \\right)$$\n",
    "  \n",
    "where $X_k$ denotes the the kth column vector of themodel design matrix $X$ formed from the first 15 data points and $\\gamma$ is a parameter that we will vary within these simulations that reflects the degree to which the last data point has higher leverage than the first set of data points. We then construct $y_i\\sim\\mathcal{N}\\left( \\beta^\\top x_i,\\sigma^2 \\right)$. The parameter $\\beta$ is nonzero for 4 features: $(\\beta_{1}, \\beta_{16}, \\beta_{17}, \\beta_{18}) = S_\\Delta(1,1,-1,1)$ where $S_\\Delta encodes signal strength.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 500 repetitions and summarize performance as follows. For the selection stage, we compute the power (defined as $\\frac{\\vert j\\in M:\\beta_j\\neq0\\vert}{\\vert j\\in[p]:\\beta_j\\neq0\\vert}$) and precision (defined as $\\frac{\\vert j\\in M:\\beta_j\\neq0\\vert}{\\vert M\\vert}$) of selecting features with a nonzero parameter. For inference, we use the false coverage rate (defined as $\\frac{\\vert k\\in M:[\\beta^*(M)]_k\\not\\in \\text{CI}_k  \\vert }{\\max\\{ \\vert M \\vert ,1 \\}}$) where $\\text{CI}_k$ is the CI for $[\\beta^*(M)]_k. We also track the average CI length within the selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_sq = 1\n",
    "n = 15\n",
    "p = 20\n",
    "betas = np.zeros(p)\n",
    "\n",
    "ones = [0,16,18]\n",
    "betas[ones] = 1\n",
    "\n",
    "neg_ones = [17]\n",
    "betas[neg_ones] = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear(n = n, p = p, betas = betas, add_influential = []):\n",
    "    n_true  = n + len(add_influential)\n",
    "    X = np.zeros((n_true, p))\n",
    "    X_1_to_n = np.random.multivariate_normal(np.zeros(p), np.eye(p), n)\n",
    "    X[:n,:] = X_1_to_n\n",
    "    if len(add_influential) > 0:\n",
    "        baseline = X_1_to_n.max(axis = 0)\n",
    "        for i in range(len(add_influential)):\n",
    "            X[(n - 1) + i,:] = baseline * add_influential[i]\n",
    "    \n",
    "    Y = np.random.normal(0, 1, n_true) + X @ betas\n",
    "    sd = 1\n",
    "    Sigma = np.eye(p)\n",
    "    return X,Y, sd, Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV, ElasticNet, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from collections import namedtuple\n",
    "\n",
    "methods = namedtuple('methods', ['masking', 'full', 'split', 'mysplit','loocv'])\n",
    "method_results = namedtuple('method_results', ['CIs', 'projected', 'selected'])\n",
    "\n",
    "def alpha1se_lasso(X, Y, cv = 10):\n",
    "    model = ElasticNetCV(cv= cv, l1_ratio=1,max_iter=100000).fit(X,Y)\n",
    "    mean_mse = np.mean(model.mse_path_, axis=1)\n",
    "    alpha_min_index = np.argmin(mean_mse)\n",
    "    mse_alpha_min = model.mse_path_[alpha_min_index,:]\n",
    "    std_error = np.std(mse_alpha_min, ddof=1) / np.sqrt(model.mse_path_.shape[1])\n",
    "    threshold = mean_mse[alpha_min_index] + std_error\n",
    "    alpha_1se_indexes = np.where(mean_mse <= threshold)[0]\n",
    "    alpha_1se = model.alphas_[alpha_1se_indexes[0]]\n",
    "    return alpha_1se\n",
    "    #return model.alpha_\n",
    "    \n",
    "\n",
    "def calculate_experiment_results(X, Y, selected, betas, Sigma, scale):\n",
    "    if len(selected) == 0:\n",
    "        return method_results(None, None, None)\n",
    "    \n",
    "    infer_model = LinearRegression().fit(X[:,selected], Y)\n",
    "    \n",
    "    ## 95% confidence intervals for the betas\n",
    "    CIs = np.zeros((len(selected), 2))\n",
    "    for i in range(len(selected)):\n",
    "        X_i = X[:,selected[i]]\n",
    "        X_not_i = np.delete(X[:,selected], i, axis = 1)\n",
    "        beta_i = infer_model.coef_[i]\n",
    "        sigma_sq_i = np.sum((Y - infer_model.predict(X[:,selected]))**2) / (len(Y) - len(selected))\n",
    "        sigma_sq_not_i = np.sum((Y - infer_model.predict(X_not_i))**2) / (len(Y) - len(selected) - 1)\n",
    "        t = beta_i / np.sqrt(sigma_sq_i * np.linalg.inv(X_i.T @ X_i)[0,0])\n",
    "        t_crit = 2.228\n",
    "        CIs[i,0] = beta_i - t_crit * np.sqrt(sigma_sq_i * np.linalg.inv(X_i.T @ X_i)[0,0])\n",
    "        CIs[i,1] = beta_i + t_crit * np.sqrt(sigma_sq_i * np.linalg.inv(X_i.T @ X_i)[0,0])\n",
    "    \n",
    "    \n",
    "    mask = np.ones(betas.shape, bool)\n",
    "    mask[selected] = False\n",
    "        \n",
    "    projected = betas[selected]*scale + scale*betas[mask] @ Sigma[mask,:][:,selected] @ np.linalg.inv(Sigma[selected,:][:,selected])\n",
    "    return method_results(CIs, projected, selected)\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def experiment_linear(n = n, p = p, betas = betas, add_influential = []):\n",
    "    scale = 1\n",
    "    X, Y, sd, Sigma = generate_linear(n = n, p = p, betas = betas, add_influential = add_influential)\n",
    "    n += len(add_influential)\n",
    "    ################ Masking ########################################################\n",
    "    sd_z = sd\n",
    "    noise = np.random.normal(0, sd_z, n)\n",
    "    g_Y = Y + noise\n",
    "    h_Y = Y - noise\n",
    "    masked_alpha_1se = alpha1se_lasso(X, g_Y)\n",
    "    \n",
    "    masked_lasso = ElasticNet(alpha=masked_alpha_1se, l1_ratio=1, max_iter=100000).fit(X,g_Y)\n",
    "    \n",
    "    masked_selected = np.where(masked_lasso.coef_ != 0)[0]\n",
    "    masked_results = calculate_experiment_results(X, h_Y, masked_selected, betas, Sigma, scale)\n",
    "    #################################################################################\n",
    "    ################ Full ###########################################################\n",
    "    full_alpha_1se = alpha1se_lasso(X, Y)\n",
    "    full_lasso = ElasticNet(alpha=full_alpha_1se, l1_ratio=1, max_iter=100000).fit(X,Y)\n",
    "    full_selected = np.where(full_lasso.coef_ != 0)[0]\n",
    "    full_results = calculate_experiment_results(X, Y, full_selected, betas, Sigma, scale)\n",
    "    #################################################################################\n",
    "    ################ Split ##########################################################\n",
    "    ## Split the data into training and testing, using binomial(1, 0.5) to decide\n",
    "    ## which group each observation belongs to\n",
    "    split = np.random.binomial(1, 0.5, n)\n",
    "    if split.sum() < 10:\n",
    "        split_results = method_results(None, None, None)\n",
    "    else:\n",
    "        X_test, X_train = [X[split == 0,:], X[split == 1,:]]\n",
    "        Y_test, Y_train = [Y[split == 0], Y[split == 1]]\n",
    "        if split.sum() < 10:\n",
    "            try:\n",
    "                split_alpha_1se = alpha1se_lasso(X_train, Y_train, cv = 8)\n",
    "            except:\n",
    "                split_alpha_1se = -10\n",
    "        else:\n",
    "            split_alpha_1se = alpha1se_lasso(X_train, Y_train)\n",
    "        if split_alpha_1se == -10:\n",
    "            split_results = method_results(None, None, None)\n",
    "        else:\n",
    "            split_lasso = ElasticNet(alpha=split_alpha_1se, l1_ratio=1, max_iter=100000).fit(X_train,Y_train)\n",
    "            split_selected = np.where(split_lasso.coef_ != 0)[0]\n",
    "            split_results = calculate_experiment_results(X_train, Y_train, split_selected, betas, Sigma, scale)\n",
    "    #################################################################################\n",
    "    ################ Split 2 ########################################################\n",
    "    if split.sum() < 8:\n",
    "        ## flip 0 and 1's in split\n",
    "        split = 1 - split\n",
    "    X_test, X_train = [X[split == 0,:], X[split == 1,:]]\n",
    "    Y_test, Y_train = [Y[split == 0], Y[split == 1]]\n",
    "    split2_alpha_1se = alpha1se_lasso(X_train, Y_train, cv = 8)\n",
    "    split2_lasso = ElasticNet(alpha=split2_alpha_1se, l1_ratio=1, max_iter=100000).fit(X_train,Y_train)\n",
    "    split2_selected = np.where(split2_lasso.coef_ != 0)[0]\n",
    "    split2_results = calculate_experiment_results(X_train, Y_train, split2_selected, betas, Sigma, scale)\n",
    "    ##############################################################################\n",
    "    ################ LOOCV #######################################################\n",
    "    ## Leave one out cross validation\n",
    "    ## randomly pick a point to leave out from 0 to n-1\n",
    "    left_out_index = np.random.choice(n)\n",
    "    X_train = np.delete(X, left_out_index, axis = 0)\n",
    "    Y_train = np.delete(Y, left_out_index)\n",
    "    loocv_alpha_1se = alpha1se_lasso(X_train, Y_train)\n",
    "    loocv_lasso = ElasticNet(alpha=loocv_alpha_1se, l1_ratio=1, max_iter=100000).fit(X_train,Y_train)\n",
    "    loocv_selected = np.where(loocv_lasso.coef_ != 0)[0]\n",
    "    loocv_results = calculate_experiment_results(X_train, Y_train, loocv_selected, betas, Sigma, scale)\n",
    "    return methods(masked_results, full_results, split_results, split2_results, loocv_results)    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Leverage 2 - Run 500: 100%|██████████| 500/500 [01:55<00:00,  4.31it/s]\n",
      "Leverage 3 - Run 500: 100%|██████████| 500/500 [02:18<00:00,  3.62it/s]\n",
      "Leverage 4 - Run 500: 100%|██████████| 500/500 [02:36<00:00,  3.19it/s]\n",
      "Leverage 5 - Run 500: 100%|██████████| 500/500 [02:48<00:00,  2.97it/s]\n",
      "Leverage 6 - Run 500: 100%|██████████| 500/500 [02:53<00:00,  2.88it/s]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(200253431)\n",
    "results_dict = {}\n",
    "runs = 500\n",
    "for lev in range(2, 7):\n",
    "    results_dict[lev] = []\n",
    "    ##\n",
    "    pbar = tqdm.tqdm(range(runs))\n",
    "    for i in pbar:\n",
    "        with np.errstate(divide='ignore'):\n",
    "            results_dict[lev].append(experiment_linear(add_influential = [lev]))\n",
    "        ## update progress bar text\n",
    "        pbar.set_description(f\"Leverage {lev} - Run {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dict = {}\n",
    "method_names = ['masking', 'full', 'split', 'mysplit', \"loocv\"]\n",
    "CI_length_dict = {method: {lev: [] for lev in range(2,7)} for method in method_names}\n",
    "for lev in range(2,7):\n",
    "    for i in range(runs):\n",
    "        for method in method_names:\n",
    "            if getattr(results_dict[lev][i], method).CIs is None:\n",
    "                continue\n",
    "            CI_array = np.array(getattr(results_dict[lev][i], method).CIs)\n",
    "            #try:\n",
    "            #    CI_length = np.nanmean(CI_array[:,1] - CI_array[:,0])\n",
    "            #except:\n",
    "            #    print(CI_length)\n",
    "            CI_length_dict[method][lev].extend((CI_array[:,1] - CI_array[:,0]))\n",
    "            #for j in range(len(CI_length)):\n",
    "            #    if not np.isnan(CI_length[j]):\n",
    "            #        CI_length_dict[method][lev].append(CI_length[j])\n",
    "            #if not np.isnan(CI_length):\n",
    "            #    CI_length_dict[method][lev].append(CI_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[2][1].split.CIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.78572107,  1.65851177],\n",
       "       [ 0.0802422 ,  3.09773544],\n",
       "       [-1.33573541,  2.45152782],\n",
       "       [-1.73953333,  2.13429813]])"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict[3][4].mysplit.CIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leverage 2\n",
      "masking: 4.395066238456812\n",
      "Length: 2394\n",
      "full: 1.3474945703179209\n",
      "Length: 2889\n",
      "split: 1.882967593814348\n",
      "Length: 276\n",
      "mysplit: 2.242569023570277\n",
      "Length: 1287\n",
      "loocv: 1.3365935120512207\n",
      "Length: 2734\n",
      "\n",
      "\n",
      "Leverage 3\n",
      "masking: 4.133690409890507\n",
      "Length: 2769\n",
      "full: 1.774718331609656\n",
      "Length: 3236\n",
      "split: 2.1136942363312516\n",
      "Length: 346\n",
      "mysplit: 2.4519119409606436\n",
      "Length: 1421\n",
      "loocv: 1.8586667150520901\n",
      "Length: 3004\n",
      "\n",
      "\n",
      "Leverage 4\n",
      "masking: 4.616462451328408\n",
      "Length: 2931\n",
      "full: 2.099303094558188\n",
      "Length: 3227\n",
      "split: 2.4046248129829073\n",
      "Length: 383\n",
      "mysplit: 3.2036274219436964\n",
      "Length: 1385\n",
      "loocv: 1.9057700352790232\n",
      "Length: 3114\n",
      "\n",
      "\n",
      "Leverage 5\n",
      "masking: 3.817100800680296\n",
      "Length: 2986\n",
      "full: 1.9800338713905286\n",
      "Length: 3217\n",
      "split: 2.503251229299672\n",
      "Length: 422\n",
      "mysplit: 2.7641810964513227\n",
      "Length: 1502\n",
      "loocv: 1.7826174890972222\n",
      "Length: 2876\n",
      "\n",
      "\n",
      "Leverage 6\n",
      "masking: 4.052516883006152\n",
      "Length: 2993\n",
      "full: 1.8156299839199117\n",
      "Length: 3183\n",
      "split: 2.3027788172648855\n",
      "Length: 458\n",
      "mysplit: 3.2253590266862058\n",
      "Length: 1528\n",
      "loocv: 1.7799006911865582\n",
      "Length: 3018\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(CI_length_dict[\"masking\"][2].mean())\n",
    "#print(CI_length_dict[\"full\"][2].mean())\n",
    "#print(CI_length_dict[\"split\"][2].mean())\n",
    "#print(CI_length_dict[\"mysplit\"][2].mean())\n",
    "#print(CI_length_dict[\"loocv\"][2].mean())\n",
    "for lev in range(2,7):\n",
    "    print(f\"Leverage {lev}\")\n",
    "    for method in method_names:\n",
    "        print(f\"{method}: {np.nanmean(np.array(CI_length_dict[method][lev]))}\")\n",
    "        ## print length of array\n",
    "        print(f\"Length: {len(CI_length_dict[method][lev])}\")\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
