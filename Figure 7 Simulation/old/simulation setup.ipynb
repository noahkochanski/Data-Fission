{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fission\n",
    "## Conducting Inference\n",
    "We conduct inference on some vector $Y$ given a set of covariates $X$ and a known covariance matrix  $\\Sigma = \\sigma^2I_n$ as follows:\n",
    "1. Decompose $y_i$ into $f(y_i) = y_i - Z_i$ and $g(y_i) = y_i + Z_i$ where $Z_i\\sim\\mathcal{N}(0,\\sigma^2)$\n",
    "2. Fit $f(y_i)$ using LASSO to select features, denoted as $M\\subseteq [p]$ (tuning parameter $\\lambda$ by 1 standard deviation rule)\n",
    "3. Fit $g(y_i)$ by linear regression without regularization using only the selected features.\n",
    "4. Construct CIs for the coeffecients trained in step 3. each at level $\\alpha$ using Theorem 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorem 2\n",
    "Let \n",
    "$$\\hat\\beta(M) = \\argmin_{\\tilde\\beta} = || g(Y) - X_M\\tilde\\beta||^2 = \\left( X_M^\\top X_M \\right)^{-1} X_M^\\top g(Y)$$\n",
    "\n",
    "and for $\\mu = \\mathbb{E}[Y\\mid X]\\in\\mathbb{R}^n$ (which is a fixed unknown quantity), \n",
    "\n",
    "$$\\beta^*(M) = \\argmin_{\\tilde\\beta} = \\mathbb{E}\\left[ || Y - X_M\\tilde\\beta ||^2 \\right] = \\left( X_M^\\top X_M \\right)^{-1} X_M^\\top \\mu.$$\n",
    "\n",
    "Then, \n",
    "\n",
    "$$\\hat\\beta(M) \\sim \\mathcal{N} \\left( \\beta^*(M), \\left( 1 +\\tau^{-2} \\right)\\left( X_M^\\top X_M\\right)^{-1} X_M^\\top \\Sigma X_M \\left( X_M^\\top X_M\\right)^{-1} \\right)$$\n",
    "\n",
    "Furthermore, we can form a $1-\\alpha$ CI for the $k$th element of $\\beta^*(M)$ as \n",
    "\n",
    "$$\\hat\\beta(M)\\pm z_{\\alpha/2}\\sqrt{\\left( 1 + \\tau^{-2} \\right) \\left[ \\left( X^\\top_M X_M\\right)^{-1} X_M^\\top\\Sigma X_M \\left( X_M^\\top X_M\\right)^{-1} \\right]_{kk} }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Setup: \n",
    "\n",
    "\"We choose $\\sigma^2 = 1$ and generate $n = 16$ data points with $p = 20$ covariates. For the first 15 data points, we have an associated vector of covariates  $x_i\\in\\mathbb{R}^p$ generated from independent Gaussians. The last data point, which we denote $x_\\text{lev}$, is generated in such a way as to ensure it is likely to bemore influential than the remaining observations due to having much larger leverage. We define \n",
    "\n",
    "$$x_\\text{lev} = \\gamma\\left( \\vert X_1 \\vert_\\infty, \\dots, \\vert X_p \\vert_\\infty \\right)$$\n",
    "  \n",
    "where $X_k$ denotes the the kth column vector of themodel design matrix $X$ formed from the first 15 data points and $\\gamma$ is a parameter that we will vary within these simulations that reflects the degree to which the last data point has higher leverage than the first set of data points. We then construct $y_i\\sim\\mathcal{N}\\left( \\beta^\\top x_i,\\sigma^2 \\right)$. The parameter $\\beta$ is nonzero for 4 features: $(\\beta_{1}, \\beta_{16}, \\beta_{17}, \\beta_{18}) = S_\\Delta(1,1,-1,1)$ where $S_\\Delta encodes signal strength.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 500 repetitions and summarize performance as follows. For the selection stage, we compute the power (defined as $\\frac{\\vert j\\in M:\\beta_j\\neq0\\vert}{\\vert j\\in[p]:\\beta_j\\neq0\\vert}$) and precision (defined as $\\frac{\\vert j\\in M:\\beta_j\\neq0\\vert}{\\vert M\\vert}$) of selecting features with a nonzero parameter. For inference, we use the false coverage rate (defined as $\\frac{\\vert k\\in M:[\\beta^*(M)]_k\\not\\in \\text{CI}_k  \\vert }{\\max\\{ \\vert M \\vert ,1 \\}}$) where $\\text{CI}_k$ is the CI for $[\\beta^*(M)]_k. We also track the average CI length within the selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_sq = 1\n",
    "n = 15\n",
    "p = 20\n",
    "betas = np.zeros(p)\n",
    "\n",
    "ones = [0,16,18]\n",
    "betas[ones] = 1\n",
    "\n",
    "neg_ones = [17]\n",
    "betas[neg_ones] = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear(n = n, p = p, betas = betas, add_influential = []):\n",
    "    n_true  = n + len(add_influential)\n",
    "    X = np.zeros((n_true, p))\n",
    "    X_1_to_n = np.random.multivariate_normal(np.zeros(p), np.eye(p), n)\n",
    "    X[:n,:] = X_1_to_n\n",
    "    if len(add_influential) > 0:\n",
    "        baseline = X_1_to_n.max(axis = 0)\n",
    "        for i in range(len(add_influential)):\n",
    "            X[(n - 1) + i,:] = baseline * add_influential[i]\n",
    "    \n",
    "    Y = np.random.normal(0, 1, n_true) + X @ betas\n",
    "    sd = 1\n",
    "    Sigma = np.eye(p)\n",
    "    cluster = np.arange(n_true)\n",
    "    return X,Y, sd, Sigma, cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV, ElasticNet\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from collections import namedtuple\n",
    "\n",
    "methods = namedtuple('methods', ['masking', 'full', 'split', 'mysplit','loocv'])\n",
    "method_results = namedtuple('method_results', ['CIs', 'projected', 'selected'])\n",
    "\n",
    "def alpha1se_lasso(X, Y, cv = 10):\n",
    "    model = ElasticNetCV(cv= cv, l1_ratio=1,max_iter=100000).fit(X,Y)\n",
    "    mean_mse = np.mean(model.mse_path_, axis=1)\n",
    "    alpha_min_index = np.argmin(mean_mse)\n",
    "    mse_alpha_min = model.mse_path_[alpha_min_index,:]\n",
    "    std_error = np.std(mse_alpha_min, ddof=1) / np.sqrt(model.mse_path_.shape[1])\n",
    "    threshold = mean_mse[alpha_min_index] + std_error\n",
    "    alpha_1se_indexes = np.where(mean_mse <= threshold)[0]\n",
    "    alpha_1se = model.alphas_[alpha_1se_indexes[0]]\n",
    "    return alpha_1se\n",
    "    #return model.alpha_\n",
    "    \n",
    "\n",
    "def calculate_experiment_results(X, Y, cluster, selected, betas, Sigma, scale):\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    if len(selected) == 0:\n",
    "        return method_results(None, None, None)\n",
    "\n",
    "    if X.shape[0] < len(cluster):\n",
    "        cluster = np.arange(X.shape[0])\n",
    "        \n",
    "    infer_model = sm.OLS(Y, X[:,selected])\n",
    "    infer_results = infer_model.fit(cov_type='cluster', cov_kwds = {'groups': cluster})\n",
    "    \n",
    "    ## 95% confidence intervals for the betas\n",
    "    CIs = infer_results.conf_int()\n",
    "    \n",
    "    \n",
    "    mask = np.ones(betas.shape, bool)\n",
    "    mask[selected] = False\n",
    "        \n",
    "    projected = betas[selected]*scale + scale*betas[mask] @ Sigma[mask,:][:,selected] @ np.linalg.inv(Sigma[selected,:][:,selected])\n",
    "    return method_results(CIs, projected, selected)\n",
    "    #eturn method_results(None, None, None)\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def experiment_linear(n = n, p = p, betas = betas, add_influential = []):\n",
    "    scale = 1\n",
    "    X, Y, sd, Sigma, cluster = generate_linear(n = n, p = p, betas = betas, add_influential = add_influential)\n",
    "    n += len(add_influential)\n",
    "    ################ Masking ########################################################\n",
    "    sd_z = sd\n",
    "    noise = np.random.normal(0, sd_z, n)\n",
    "    g_Y = Y + noise\n",
    "    h_Y = Y - noise\n",
    "    masked_alpha_1se = alpha1se_lasso(X, g_Y)\n",
    "    \n",
    "    masked_lasso = ElasticNet(alpha=masked_alpha_1se, l1_ratio=1, max_iter=100000).fit(X,g_Y)\n",
    "    \n",
    "    masked_selected = np.where(masked_lasso.coef_ != 0)[0]\n",
    "    masked_results = calculate_experiment_results(X, h_Y, cluster, masked_selected, betas, Sigma, scale)\n",
    "    #################################################################################\n",
    "    ################ Full ###########################################################\n",
    "    full_alpha_1se = alpha1se_lasso(X, Y)\n",
    "    full_lasso = ElasticNet(alpha=full_alpha_1se, l1_ratio=1, max_iter=100000).fit(X,Y)\n",
    "    full_selected = np.where(full_lasso.coef_ != 0)[0]\n",
    "    full_results = calculate_experiment_results(X, Y, cluster, full_selected, betas, Sigma, scale)\n",
    "    #################################################################################\n",
    "    ################ Split ##########################################################\n",
    "    ## Split the data into training and testing, using binomial(1, 0.5) to decide\n",
    "    ## which group each observation belongs to\n",
    "    split = np.random.binomial(1, 0.5, n)\n",
    "    split = np.zeros(n)\n",
    "    split[0:2] = 1\n",
    "    if split.sum() > 10:\n",
    "        split_results = method_results(None, None, None)\n",
    "    else:\n",
    "        X_test, X_train = X[split == 0,:], X[split == 1,:]\n",
    "        Y_test, Y_train = Y[split == 0], Y[split == 1]\n",
    "        split_alpha_1se = alpha1se_lasso(X_train, Y_train)\n",
    "        if split_alpha_1se == -10:\n",
    "            split_results = method_results(None, None, None)\n",
    "        else:\n",
    "            split_lasso = ElasticNet(alpha=split_alpha_1se, l1_ratio=1, max_iter=100000).fit(X_test,Y_test,nsplits = 2)\n",
    "            split_selected = np.where(split_lasso.coef_ != 0)[0]\n",
    "            split_results = calculate_experiment_results(X_train, Y_train, cluster, split_selected, betas, Sigma, scale)\n",
    "    #################################################################################\n",
    "    ################ Split 2 ########################################################\n",
    "    if split.sum() < 8:\n",
    "        ## flip 0 and 1's in split\n",
    "        split = 1 - split\n",
    "    X_test, X_train = [X[split == 0,:], X[split == 1,:]]\n",
    "    Y_test, Y_train = [Y[split == 0], Y[split == 1]]\n",
    "    split2_alpha_1se = alpha1se_lasso(X_train, Y_train, cv = 8)\n",
    "    split2_lasso = ElasticNet(alpha=split2_alpha_1se, l1_ratio=1, max_iter=100000).fit(X_test,Y_test)\n",
    "    split2_selected = np.where(split2_lasso.coef_ != 0)[0]\n",
    "    split2_results = calculate_experiment_results(X_train, Y_train, cluster, split2_selected, betas, Sigma, scale)\n",
    "    ##############################################################################\n",
    "    ################ LOOCV #######################################################\n",
    "    ## Leave one out cross validation\n",
    "    ## randomly pick a point to leave out from 0 to n-1\n",
    "    left_out_index = np.random.choice(n, size =2 , replace = False)\n",
    "    X_train = np.delete(X, left_out_index, axis = 0)\n",
    "    Y_train = np.delete(Y, left_out_index)\n",
    "    loocv_alpha_1se = alpha1se_lasso(X_train, Y_train)\n",
    "    loocv_lasso = ElasticNet(alpha=loocv_alpha_1se, l1_ratio=1, max_iter=100000).fit(X_train,Y_train)\n",
    "    loocv_selected = np.where(loocv_lasso.coef_ != 0)[0]\n",
    "    loocv_results = calculate_experiment_results(X[left_out_index,:], Y[left_out_index], cluster, loocv_selected, betas, Sigma, scale)\n",
    "    return methods(masked_results, full_results, split_results, split2_results, loocv_results)    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 20)\n",
      "(16,)\n",
      "(16, 20)\n",
      "(16,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=10 greater than the number of samples: n_samples=2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[777], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexperiment_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43madd_influential\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_testing.py:158\u001b[0m, in \u001b[0;36m_IgnoreWarnings.__call__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m    157\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategory)\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[776], line 81\u001b[0m, in \u001b[0;36mexperiment_linear\u001b[0;34m(n, p, betas, add_influential)\u001b[0m\n\u001b[1;32m     79\u001b[0m X_test, X_train \u001b[38;5;241m=\u001b[39m X[split \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m,:], X[split \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m,:]\n\u001b[1;32m     80\u001b[0m Y_test, Y_train \u001b[38;5;241m=\u001b[39m Y[split \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m], Y[split \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 81\u001b[0m split_alpha_1se \u001b[38;5;241m=\u001b[39m \u001b[43malpha1se_lasso\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_alpha_1se \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m     83\u001b[0m     split_results \u001b[38;5;241m=\u001b[39m method_results(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[776], line 12\u001b[0m, in \u001b[0;36malpha1se_lasso\u001b[0;34m(X, Y, cv)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21malpha1se_lasso\u001b[39m(X, Y, cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mElasticNetCV\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     mean_mse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(model\u001b[38;5;241m.\u001b[39mmse_path_, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m     alpha_min_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmin(mean_mse)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:1728\u001b[0m, in \u001b[0;36mLinearModelCV.fit\u001b[0;34m(self, X, y, sample_weight, **params)\u001b[0m\n\u001b[1;32m   1725\u001b[0m     routed_params\u001b[38;5;241m.\u001b[39msplitter \u001b[38;5;241m=\u001b[39m Bunch(split\u001b[38;5;241m=\u001b[39mBunch())\n\u001b[1;32m   1727\u001b[0m \u001b[38;5;66;03m# Compute path for all folds and compute MSE to get the best alpha\u001b[39;00m\n\u001b[0;32m-> 1728\u001b[0m folds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1729\u001b[0m best_mse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;66;03m# We do a double for loop folded in one, in order to be able to\u001b[39;00m\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;66;03m# iterate in parallel on l1_ratio and folds\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_split.py:370\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    368\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m n_samples:\n\u001b[0;32m--> 370\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    371\u001b[0m         (\n\u001b[1;32m    372\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have number of splits n_splits=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples: n_samples=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    374\u001b[0m         )\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits, n_samples)\n\u001b[1;32m    375\u001b[0m     )\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups):\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot have number of splits n_splits=10 greater than the number of samples: n_samples=2."
     ]
    }
   ],
   "source": [
    "experiment_linear(add_influential=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Leverage 2 - Run 500: 100%|██████████| 500/500 [02:25<00:00,  3.44it/s]\n",
      "Leverage 3 - Run 500: 100%|██████████| 500/500 [02:59<00:00,  2.78it/s]\n",
      "Leverage 4 - Run 500: 100%|██████████| 500/500 [03:35<00:00,  2.32it/s]\n",
      "Leverage 5 - Run 500: 100%|██████████| 500/500 [04:06<00:00,  2.03it/s]\n",
      "Leverage 6 - Run 500: 100%|██████████| 500/500 [03:26<00:00,  2.42it/s]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(200253431)\n",
    "results_dict = {}\n",
    "runs = 500\n",
    "for lev in range(2, 7):\n",
    "    results_dict[lev] = []\n",
    "    ##\n",
    "    pbar = tqdm.tqdm(range(runs))\n",
    "    for i in pbar:\n",
    "        with np.errstate(divide='ignore'):\n",
    "            results_dict[lev].append(experiment_linear(add_influential = [lev]))\n",
    "        ## update progress bar text\n",
    "        pbar.set_description(f\"Leverage {lev} - Run {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dict = {}\n",
    "method_names = ['masking', 'full', 'split', 'mysplit', \"loocv\"]\n",
    "CI_length_dict = {method: {lev: [] for lev in range(2,7)} for method in method_names}\n",
    "for lev in range(2,7):\n",
    "    for i in range(runs):\n",
    "        for method in method_names:\n",
    "            if getattr(results_dict[lev][i], method).CIs is None:\n",
    "                continue\n",
    "            CI_array = np.array(getattr(results_dict[lev][i], method).CIs)\n",
    "            #try:\n",
    "            #    CI_length = np.nanmean(CI_array[:,1] - CI_array[:,0])\n",
    "            #except:\n",
    "            #    print(CI_length)\n",
    "            CI_length_dict[method][lev].extend((CI_array[:,1] - CI_array[:,0]))\n",
    "            #for j in range(len(CI_length)):\n",
    "            #    if not np.isnan(CI_length[j]):\n",
    "            #        CI_length_dict[method][lev].append(CI_length[j])\n",
    "            #if not np.isnan(CI_length):\n",
    "            #    CI_length_dict[method][lev].append(CI_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[2][1].split.CIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.78572107,  1.65851177],\n",
       "       [ 0.0802422 ,  3.09773544],\n",
       "       [-1.33573541,  2.45152782],\n",
       "       [-1.73953333,  2.13429813]])"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict[3][4].mysplit.CIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leverage 2\n",
      "masking: 4.395066238456812\n",
      "Length: 2394\n",
      "full: 1.3474945703179209\n",
      "Length: 2889\n",
      "split: 1.882967593814348\n",
      "Length: 276\n",
      "mysplit: 2.242569023570277\n",
      "Length: 1287\n",
      "loocv: 1.3365935120512207\n",
      "Length: 2734\n",
      "\n",
      "\n",
      "Leverage 3\n",
      "masking: 4.133690409890507\n",
      "Length: 2769\n",
      "full: 1.774718331609656\n",
      "Length: 3236\n",
      "split: 2.1136942363312516\n",
      "Length: 346\n",
      "mysplit: 2.4519119409606436\n",
      "Length: 1421\n",
      "loocv: 1.8586667150520901\n",
      "Length: 3004\n",
      "\n",
      "\n",
      "Leverage 4\n",
      "masking: 4.616462451328408\n",
      "Length: 2931\n",
      "full: 2.099303094558188\n",
      "Length: 3227\n",
      "split: 2.4046248129829073\n",
      "Length: 383\n",
      "mysplit: 3.2036274219436964\n",
      "Length: 1385\n",
      "loocv: 1.9057700352790232\n",
      "Length: 3114\n",
      "\n",
      "\n",
      "Leverage 5\n",
      "masking: 3.817100800680296\n",
      "Length: 2986\n",
      "full: 1.9800338713905286\n",
      "Length: 3217\n",
      "split: 2.503251229299672\n",
      "Length: 422\n",
      "mysplit: 2.7641810964513227\n",
      "Length: 1502\n",
      "loocv: 1.7826174890972222\n",
      "Length: 2876\n",
      "\n",
      "\n",
      "Leverage 6\n",
      "masking: 4.052516883006152\n",
      "Length: 2993\n",
      "full: 1.8156299839199117\n",
      "Length: 3183\n",
      "split: 2.3027788172648855\n",
      "Length: 458\n",
      "mysplit: 3.2253590266862058\n",
      "Length: 1528\n",
      "loocv: 1.7799006911865582\n",
      "Length: 3018\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(CI_length_dict[\"masking\"][2].mean())\n",
    "#print(CI_length_dict[\"full\"][2].mean())\n",
    "#print(CI_length_dict[\"split\"][2].mean())\n",
    "#print(CI_length_dict[\"mysplit\"][2].mean())\n",
    "#print(CI_length_dict[\"loocv\"][2].mean())\n",
    "for lev in range(2,7):\n",
    "    print(f\"Leverage {lev}\")\n",
    "    for method in method_names:\n",
    "        print(f\"{method}: {np.nanmean(np.array(CI_length_dict[method][lev]))}\")\n",
    "        ## print length of array\n",
    "        print(f\"Length: {len(CI_length_dict[method][lev])}\")\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
